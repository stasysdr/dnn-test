{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stasysdr/dnn-test/blob/main/Artificial_neural_network_demo_Number_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This is a demonstration of artificial neural networks - how the code looks like, how the models of deep neural networks are created, trained, and tested.\n",
        "\n",
        "The purpose of this demonstration is to illustrate the following legal points:\n",
        "* Artificial Neural Network coding can be simple and technical, where copyright protection of the code would not be most important\n",
        "* where are \"hyperparameters\" of the ANN in the code and how they look like. They can be the hardest to figure out and therefore they qualify for legal protection as trade secrets\n",
        "* how the weights look like in the trained ANN, as they may be protected as database\n",
        "\n",
        "In this demonstration the deep neural network model will have two convolutional layers of artificial neurons, one dense layer of artificial neurons, and final layer for classification.\n",
        "\n",
        "The model will be use for computer vision task - to recognise digits (from 0 to 9) in the images."
      ],
      "metadata": {
        "id": "eYWH-D7eCn3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most of the code is already written\n",
        "\n",
        "As you will see, there is little coding required for machine learning task.\n",
        "\n",
        "This is because the code is already packed in so called \"libraries\", and you can use large portions of that just by asking to execute required functions, which have short names.\n",
        "\n",
        "Most such libraries are open source and free to use.\n",
        "\n",
        "tensorflow - is the core library developed by Google for internal use, but then released to the public. There are more such libraries, such as pytorch. In this demonstration we will use tensorflow.\n",
        "\n",
        "keras - is a high-level library built on top of tensorflow. It makes easier to create models faster, as less coding is required. However, using tensorflow, would allow you to fine-tune your machine learning program, but require more code.\n",
        "\n",
        "Let's see how these libraries are added (imported), when we write the code in python language.\n",
        "\n",
        "Play the code below. You may receive warning regarding access. ***The code in this demo does not need and will not ask for access to your files.***"
      ],
      "metadata": {
        "id": "OEoZxxmHEBQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX-88XByM-uJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, utils, Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, these are amazingly simple lines, but they will let to access prewritten code in your python programme later.\n",
        "\n",
        "We use import \"long name\" as \"short name\", because later in code its easier to repeatedly refer to \"tf\", rather than \"tensorflow\".\n",
        "\n",
        "We will only use parts of keras library - models, layers, utils, so we import only those parts, which makes accessing those parts in the code shorter."
      ],
      "metadata": {
        "id": "hCk5IEhWeNDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also need some other libraries, which help display images, and plot graphs."
      ],
      "metadata": {
        "id": "XP9O-BBOfEmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSvBALWSnnTr"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt # This library will plot graphs to visualise data and images\n",
        "\n",
        "import pathlib\n",
        "import numpy as np # Library needed for calculations with image pixel values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the data for training the model\n",
        "\n",
        "First, we have to get data - many examples of images containing digits from 1 to 10.\n",
        "\n",
        "The data is publicly availabe at the address you see below. You can download it yourself to inspect what's inside.\n",
        "\n",
        "Let's play the cell bellow to retrieve the data and unpack it into this Colab environment."
      ],
      "metadata": {
        "id": "oRPf0J8KNlYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX2ftJy40NaE"
      },
      "outputs": [],
      "source": [
        "dataset_url = \"https://github.com/stasysdr/dnn-test/raw/main/digits.tgz\" # you can copy paste this address and get the archived images used in this demo\n",
        "data_dir = utils.get_file('digits.tar', origin=dataset_url, extract=True,cache_dir=\"/content\")\n",
        "data_dir = pathlib.Path(data_dir).with_suffix('')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's count how many images are there in the obtained dataset. Should be 796."
      ],
      "metadata": {
        "id": "NrxsacABOZeO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93bfJae60hlj"
      },
      "outputs": [],
      "source": [
        "image_count = len(list(data_dir.glob('*/*.png')))\n",
        "print(image_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the data\n",
        "\n",
        "Now let's see what the images contain, and what our neural network will have to recognise.\n",
        "\n",
        "Let's display one image for each class. As we will classify the images into 10 classes (from 0 to 9), we have to see 10 images."
      ],
      "metadata": {
        "id": "ciQHYLQTO__Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC0sxBUWdaRD"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(ncols=10, nrows=1, figsize=(10,1),)\n",
        "for i in range(10):\n",
        "    dir = str(i)+\"/*\"\n",
        "    digit = list(data_dir.glob(dir))\n",
        "    im = Image.open(digit[i])\n",
        "    axs[i].imshow(im)\n",
        "    axs[i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a closer look at one image of a digit \"1\"."
      ],
      "metadata": {
        "id": "4MxpNOSyRGgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open(list(data_dir.glob('1/tet1-4.png'))[0])\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(im)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8dEbV202P-hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We humans see the images visually, but if you expand the image enough, you start seeing pixels. This is an image of total 18 pixels vertically and 20 pixels horizontally.\n",
        "\n",
        "Of course, computers do not, and our neural network model will not, \"see\" visually, but they understand numbers.\n",
        "\n",
        "For a computer, each pixel is a number. See for yourself.\n"
      ],
      "metadata": {
        "id": "mGanOl21Pqon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ima = np.asarray(im.convert('RGB'))\n",
        "ima = np.dot(ima[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "fig, ax = plt.subplots(figsize=(7.8,7.5))\n",
        "ax.imshow(im)\n",
        "for i in range(18):\n",
        "    for j in range(20):\n",
        "        ax.text(j,i,f\"{int(ima[i][j])}\", color=(\"k\" if ima[i][j]>150 else \"w\"),fontsize=8, horizontalalignment=\"center\", verticalalignment=\"center\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VJfwb6NDR6FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pixels are numbers from 0 to 255, representing colour. The neural network will make calculations with these numbers and \"learn\" to recognise the whole 'picture' they represent."
      ],
      "metadata": {
        "id": "F4gh8c4LrvL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets for training and validation\n",
        "\n",
        "Our Neural Network will train by inspecting and making calculations with the set of images, which are labelled by human, and will try to calculate which label (class) should be assigned to the image (classes are from 0 to 9).\n",
        "\n",
        "Over more and more training cycles, the neural network model will get better and better at calculating the correct labels.\n",
        "\n",
        "However, as a good practice, it is not sufficient to rely only on the results of calculations made on the data on which the model was trained, because the model has already \"seen\" those images on which it was trained.\n",
        "\n",
        "Therefore, we will split all the images into 80% for training, and 20% for validation data sets.\n",
        "\n",
        "Validation data will be used for training, but will be used after training to check how the model is doing with data it has not yet \"seen\", when training.\n",
        "\n",
        "Strangely, the \"training data\" and \"validation data\" definitions found their way even in [EU Artificial Intelligence Act](https://eur-lex.europa.eu/eli/reg/2024/1689/oj), Article 3(29), 3(30), and training the models on both those datasets is a legal requirement for data governance under Article 10:\n",
        "\n",
        "> 1.   High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.\n",
        "\n",
        " [...]\n",
        "> 3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used. Those characteristics of the data sets may be met at the level of individual data sets or at the level of a combination thereof.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mqhPIfMxrx-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating training and validation datasets\n",
        "\n",
        "Let's create a training data set.\n",
        "\n",
        "We will define the size of the images - each image used for training must have exactly the same number of pixels, so defining the size of images helps adjust the images in case they are different in size. In our case they are all the same 18 pixels height by 20 pixels width.\n",
        "\n",
        "batch_size - will define the number of images used in one batch for training cycle.  It would required a lot of computer memory to take all hundreds of images at once. Therefore, we do one training cycle with a set of 32 images.\n",
        "\n",
        "\"Batch size\" is one of the **hyperparameters**, which can be adjusted to improve training, as training may have a bit different results if we train with more or less images at a time.\n",
        "\n",
        "As adjusting hyperparameters requires manual work, they are particularly important part of a model as they may become a \"trade secret\" from legal point of view. We will return to hyperparameters later."
      ],
      "metadata": {
        "id": "VF_1T79Yy9ch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H74l2DoDI2XD"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 # defines how many images will be fed to the model at a time for one training cycle\n",
        "\n",
        "img_height = 18\n",
        "img_width = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will convert our images from colour (RBG) to grayscale format, so there is less data in each image. RGB would have three numbers to define each pixel, and grayscale only one.\n",
        "\n",
        "The images are saved as .png files in the following directory structure:\n",
        "```\n",
        "digits/\n",
        "  0/\n",
        "    tet101-3.png\n",
        "    tet102-3.png\n",
        "    ..\n",
        "  1/\n",
        "    tet1-4.png\n",
        "    ..\n",
        "  2/\n",
        "    tet10-1.png\n",
        "    tet101-2.png\n",
        "    ..\n",
        "  3/\n",
        "    tet108-4.png\n",
        "    ..\n",
        "  4/\n",
        "    tet10-2.png\n",
        "    ..\n",
        "  5/\n",
        "    tet100-2.png\n",
        "    ..\n",
        "  6/\n",
        "    tet1-1.png\n",
        "    ..\n",
        "  7/\n",
        "    ..\n",
        "  8/\n",
        "    ..\n",
        "  9/\n",
        "    ..\n",
        "```\n",
        "\n",
        "So the directory names serve as labels - all images in a directory will be assigned the relevant label, e.g. \"1\", \"7\", etc. No need to fix a label for each individual image.\n",
        "\n",
        "This is what the so called \"labelled data\" looks like."
      ],
      "metadata": {
        "id": "I_4RJY9hzhjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqo9urmpdobJ"
      },
      "outputs": [],
      "source": [
        "training_data_set, validation_data_set = utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"both\",\n",
        "  seed=123,\n",
        "  color_mode='grayscale',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, just one line of code referring to keras.utils library and using the function \"image_dataset_from_directory\" does all the job of selecting 80% of images for training, 20% of images for validation, organising them into batches of 32 images, and converting them to grayscale. This is really cool."
      ],
      "metadata": {
        "id": "TSpohPKYz9JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the data\n",
        "\n",
        "Let's now check if the labels were taken into the datasets. They are read from the names of the directories for each class of numbers."
      ],
      "metadata": {
        "id": "CC2gmrwiEC60"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0rBZcS4d3Ak"
      },
      "outputs": [],
      "source": [
        "class_names = training_data_set.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's also make sure that images in the training dataset have correct labels and are uniform. We will take 8 sample images and labels."
      ],
      "metadata": {
        "id": "YJo0w9swBtHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JN0AA92d3iE"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(7, 3.5))\n",
        "for images, labels in training_data_set.take(1):\n",
        "  for i in range(8):\n",
        "    ax = fig.add_subplot(2, 4, i + 1)\n",
        "    ax.imshow(images[i].numpy().astype(\"uint8\"),cmap=\"gray\")\n",
        "    ax.set_title(class_names[labels[i]])\n",
        "    ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep artificial neural network\n",
        "\n",
        "Here is where the magic will be happening.\n",
        "\n",
        "We will use a simple model of only four layers of artificial neurons. Not so deep network.\n",
        "\n",
        "First two layers are 'convolutional'. Do not worry about the meaning of this term. Just know that such type of layers allow us to have less artificial neurons than the number of pixels in the image, and they are very efficient in learning to analyse images.\n",
        "\n",
        "In those layers the neurons will be arranged into 3 by 3 filters. Think of a filter like a matrix:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "They will try to calculate the so called parameters, in order to find those, which predict the label of an image previously unseen by the neural network, as best and as accurate as possible.\n",
        "\n",
        "Remember, that an artificial neuron is a place in the network, where very simple calculations are happening always under the same formula:\n",
        "\n",
        "w1 * x1 + b\n",
        "\n",
        "where\n",
        "\n",
        "  w1 - trainable parameter (called 'weight')\n",
        "\n",
        "  x1 - real world feature, in our case - number representing a singl pixel of a picture\n",
        "\n",
        "  b -  bias, nothing related to 'bias' in legal sense, also a trainable parameter.\n",
        "\n",
        "After the fist iteration, the result of the formula (predicted label of image) is checked against the actual label. If there is deviation, w1 will be modified a little, to try to find better parameter w1. This is how the network is trained, and how the machines learn - they try to find best parameters (weights).\n",
        "\n",
        "Let's take a look at the code. As you see, with keras it takes one line of code to define a layer with 160 or one line for a layer with 4640 trainable parameters.\n",
        "\n",
        "Let's activate the code and build the model."
      ],
      "metadata": {
        "id": "aLdOTWeTJn-3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRJfWPHfevA5"
      },
      "outputs": [],
      "source": [
        "filter_size = 3 # this determines the size of a filter in the below code - 3 * 3 = 9 trainable weights\n",
        "\n",
        "model = Sequential([\n",
        "  #layers.Input(shape=(img_height, img_width, 1)),\n",
        "  layers.Rescaling(1./255, name='rescaling'),\n",
        "  layers.Conv2D(16, filter_size, activation='relu', name='first'),  # 16 filters of size 3*3 (144 weights)  + bias for each filter (16) = 160 total weights\n",
        "  layers.Conv2D(32, filter_size, activation='relu', name='antras'), # 32 filters of size 3*3 times 16 filters from previous layer + bias = 4640\n",
        "  layers.Flatten(name='flat'), # this layer puts all weights in one line, i.e. \"flattens\" the matrixes of filters from previous layers\n",
        "  layers.Dense(64, activation='relu', name='dense'), # this is a \"dense\" layer with 64 artificial neurons. They calculate ('train') total 458,752 weights and 64 biases\n",
        "  layers.Dense(len(class_names), name='classify') # final layer for classification into 10 classes, it has ten neurons, one for each class.\n",
        "], name='digitrecognition')\n",
        "\n",
        "# nummber of layers, number of filters, size of filters, activation function, are all hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is defined and stored in a variable \"model\".\n",
        "\n",
        "Now let's compile and build the model. Then we will see the summary of information about the model, the total number of trainable parameters."
      ],
      "metadata": {
        "id": "QSzTPOt6HLPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhd4B3OpfFFH"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', # adam - this is a type of optimization, which is a hyperparameter\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31XcYGp9fISl"
      },
      "outputs": [],
      "source": [
        "model.build((None,img_height, img_width, 1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is it, the model is created and built, we can now train it, but first, let's talk about \"hyperparameters\" - the very secret of the neural network."
      ],
      "metadata": {
        "id": "mTIQI_rVIJaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters as trade secrets\n",
        "\n",
        "When building an artificial neural network, we usually do not know in advance, which structure is best.\n",
        "\n",
        "* How many layers should the network have?\n",
        "\n",
        "* How many artificial neurons there should be in a layer?\n",
        "\n",
        "* If we use convolutional layer, how many filters? What size? 3x3 or 5x5? Should we use padding? What stride is best? Should we add max (or average) pooling layer?\n",
        "\n",
        "* Which activation function should be use? (in our model, we use 'relu', but is it best)?\n",
        "\n",
        "The more layers, the more neurons or filters, the more computing resources and time training will require.\n",
        "\n",
        "All these questions can be answered only by trying the network.\n",
        "\n",
        "Of course, there is experience publicly available which can provide some suggestions. But for more customised or complex applications, those questions can be answered only by building and trying the artificial networks.\n",
        "\n",
        "This is why it requires investment of time, effort, consumption of computing resources.\n",
        "\n",
        "And once those hyperparameters are found and we have a model which works best, this is the knowledge, which is developed by effort and therefore can qualify as a trade secret from legal point of view."
      ],
      "metadata": {
        "id": "9tIsabD5iLrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the artificial neural network\n",
        "\n",
        "We will now train the model, by fitting the training data set of images, 637 in total.\n",
        "\n",
        "It is not feasible to stuff all images at once, so we will feed them in batches of 32 images - total 20 batches. After each batch, the weights are updated to see how the other batch will perform. After all batches (i.e. all images) are through the training process, we complete one training cycle. The cycle is called \"Epoch\".\n",
        "\n",
        "During one training cycle, the trainable parameters are tested by calculating predictions of labels of images. If prerdicted labels are off, the parameters are adjusted to try to improve the model accuracy."
      ],
      "metadata": {
        "id": "DoRJLcqpIVf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_weights = model.get_layer(name=\"first\").get_weights() # this will keep the original weights, which are initiated to random values\n",
        "\n",
        "accr, loss, val_acc = [], [], []\n",
        "class training_logs(tf.keras.callbacks.Callback): # to keep record of training cycles, needed to print accuracy graphs later\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    self.ep = epoch\n",
        "\n",
        "  def on_batch_end(self, batch, logs=None):\n",
        "    accr.append(logs['accuracy'])\n",
        "    loss.append(logs['loss'])\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    val_acc.append(logs['val_accuracy'])"
      ],
      "metadata": {
        "id": "qGcpGHlct84Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's run 5 epochs, or training cycles."
      ],
      "metadata": {
        "id": "bvo1dvnZrhYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXpzZsfyfo6o"
      },
      "outputs": [],
      "source": [
        "epochs=5  #the number of epochs or training cycles\n",
        "history = model.fit( # this will initiate the training of the model on the tain_ds data set\n",
        "  training_data_set,\n",
        "  validation_data=validation_data_set,\n",
        "  epochs=epochs,\n",
        "  callbacks=[training_logs()],\n",
        "  verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualising the 'training' of the weights\n",
        "\n",
        "Now let's look at the weight as they were initialised into random numbers and how they changed during learning process.\n",
        "\n",
        "The code below will display values of all 160 weights of the first layer of artificial neurons before and after the training."
      ],
      "metadata": {
        "id": "a4i5qNrO85v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learned_weights = model.get_layer(name=\"first\").get_weights()\n",
        "compare_weights = {\"0\": initial_weights,\n",
        "                   \"1\": learned_weights}\n",
        "\n",
        "fig = plt.figure(figsize=(11,6), constrained_layout=True)\n",
        "subfigs = fig.subfigures(1, 2, wspace=0.05)\n",
        "\n",
        "for outerind, subfig in enumerate(subfigs.flat):\n",
        "  axs = subfig.subplots(4, 4)\n",
        "\n",
        "  subfig.suptitle('Initial random weights' if outerind == 0 else 'Weights learned by neurons in the first layer of network', y=1.05);\n",
        "  for innerind, ax in enumerate(axs.flat):\n",
        "    ax.imshow(compare_weights[str(outerind)][0][:,:,0,innerind],cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(str(int(compare_weights[str(outerind)][1][innerind]*1000)/1000), y=0, pad=-3, verticalalignment=\"top\",fontsize=7)\n",
        "    for a in range(3):\n",
        "      for b in range(3):\n",
        "        ax.text(a, b, int(compare_weights[str(outerind)][0][b,a,0,innerind]*100)/100,\n",
        "                color=(\"k\" if compare_weights[str(outerind)][0][b,a,0,innerind]>0 else \"w\"),\n",
        "                fontsize=7, horizontalalignment=\"center\", verticalalignment=\"center\");"
      ],
      "metadata": {
        "id": "pd5-2I49QR8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualise by how much the weights got trained in 3d :)\n",
        "\n",
        "There are 16 filters, having 9 weights each (3x3)."
      ],
      "metadata": {
        "id": "5YJdyJ918chJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y,z = [],[],[]\n",
        "\n",
        "for i in range(4): # Arranging x and y positions for each neuron\n",
        "  a=1\n",
        "  for k in range(12):\n",
        "    for j in range(1,4):\n",
        "      x.append([j+i*3,j+i*3])\n",
        "      y.append([a,a])\n",
        "    a = a+1\n",
        "\n",
        "for i in range(16): # Getting z length (height) for eac neuron\n",
        "  for k in range(3):\n",
        "    for l in range(3):\n",
        "      z.append( [1, 1+abs(initial_weights[0][k,l,0,i]-learned_weights[0][k,l,0,i])])\n",
        "\n",
        "# Ploting all neurons in 3d projection\n",
        "fig = plt.figure( figsize = (8, 8))\n",
        "clr = plt.colormaps['hsv'](np.random.rand(16,))\n",
        "i=0\n",
        "fig.suptitle(f\"The extent of change of weights during learning\", ha='center')\n",
        "for s in range(16): # iterating through 16 filters\n",
        "  ax = fig.add_subplot(4,4,s+1,projection='3d')\n",
        "  for t in range(9): # Iterating through 9 weights of each filter\n",
        "    ax.plot(x[i], y[i], z[i], 'o', linestyle=(0,(1,0)), linewidth=2.0, c=clr[s])\n",
        "    i=i+1\n",
        "  ax.axis(\"off\")\n",
        "  ax.set_title(f\"{s+1} filter\", fontsize=9, y=0.95)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4TdDr2NvEQNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNrpXlPcvPjJ"
      },
      "source": [
        "The height of 3d bars shows the difference between initial value and the value after training.\n",
        "\n",
        "As you see, the weights are adjusted by learning process. This is how machines \"learn\" and how they \"remember\" what they learned - they keep the weights in their \"memory\".\n",
        "\n",
        "## Weights as databases\n",
        "\n",
        "As it takes effort to create the ‘database’ of weights, the collection of this particular data qualifies as a protected database from legal point of view. Database is defined as a collection of data arranged in a systematic or methodical way and individually accessible by electronic or other means.\n",
        "\n",
        "Databases are legally protected, if there has been qualitatively and/or quantitatively a substantial investment in either the obtaining, verification or presentation of the contents of the database.\n",
        "\n",
        "As it takes intellectual effort to find the correct values of weights, and also it can require significant cost in terms of computing power and time, the system of weights learned by a neural network may be legally protected as a database.\n",
        "\n",
        "Below code will display a small fraction of the database of the trained weights - the weights of one filter of first layer of the network, i.e. 10 weights out of total 464,266."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the weights of first filter of first layer in pure numbers\n",
        "np.set_printoptions(suppress=True)\n",
        "print(f\"Initial weights:\\n{initial_weights[0][:,:,0,0]}\\nbias:{initial_weights[1][0]}\\n\")\n",
        "print(f\"Learned weights:\\n{learned_weights[0][:,:,0,0]}\\nbias:{learned_weights[1][0]}\\n\")\n",
        "print(f\"Difference (absolute):\\n{abs(learned_weights[0][:,:,0,0]-initial_weights[0][:,:,0,0])}\\nbias:{abs(learned_weights[1][0]-initial_weights[1][0])}\")"
      ],
      "metadata": {
        "id": "MHbuZmZoi1ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the trained network\n",
        "\n",
        "We will now load 20 images, which were not included in training or validation datasets, i.e. they are completely 'unseen' by the neural network.\n",
        "\n",
        "The network will calculate the labels using the weights it learned so far. We can call such calculated labels as 'predictions', because the network does not 'know' if the calculated label is actually correct.\n",
        "\n",
        "However, the network can calculate how confident the prediction is, i.e. by how much the calculated label is likely to be correct, in comparison with other labels."
      ],
      "metadata": {
        "id": "1nHCopYEk_Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imlist = list(pathlib.Path('/content/datasets/digitstest/').glob('*/*.png'))\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "  # Load image\n",
        "  img = tf.keras.utils.load_img(image_path, target_size=(img_height, img_width), color_mode='grayscale')\n",
        "  img = tf.keras.utils.img_to_array(img)\n",
        "  img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "  return img\n",
        "\n",
        "def print_model_predictions():\n",
        "\n",
        "  images = [load_and_preprocess_image(path) for path in imlist]\n",
        "  images = np.vstack(images)  # Combine images into a batch\n",
        "\n",
        "  predictions = model.predict(images, verbose=0) # Run the images through the trained model\n",
        "  predicted_labels = np.argmax(predictions, axis=1) # Calculated predicted labels\n",
        "  true_labels = [int(str(i)[29]) for i in imlist] # Read true labels from image folders\n",
        "  accuracy = np.sum(predicted_labels == true_labels)/len(true_labels) # Calculate accuracy\n",
        "\n",
        "  # Print results\n",
        "  fig = plt.figure(figsize=(7, 6))\n",
        "  fig.suptitle(f\"Predicted labels and confidence level (%) of predictions\\n\", ha='center')\n",
        "  plt.title(f\"Accuracy of the trained model - {int(accuracy*1000)/10}%\", fontsize=11, y=1.03)\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  for f in range(len(imlist)): # Loop through each evaluated image\n",
        "\n",
        "      score = tf.nn.softmax(predictions[f])\n",
        "      confidence = \"\" + str(int(1000 * np.max(score))/10)+\"%\"\n",
        "\n",
        "      label = \"\"+str(predicted_labels[f])\n",
        "      correct_label = true_labels[f] == predicted_labels[f]\n",
        "\n",
        "      ax = fig.add_subplot(4, int(len(imlist)/4), f + 1)\n",
        "      ax.imshow(Image.open(imlist[f]))\n",
        "      ax.set_title(label+\"\", loc=\"center\", fontsize=12)\n",
        "      ax.set_title(confidence, loc=\"right\", fontsize=8)\n",
        "      ax.set_title(\"correct\" if correct_label else \"false\" , loc=\"left\", fontsize=7, c=\"g\" if correct_label else \"r\")\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "\n",
        "print_model_predictions()"
      ],
      "metadata": {
        "id": "LLXszoFbT8oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You see that after training for 5 epochs, our model has an average performance - it correctly predicts labels of images only in 1 out of 2, which is quite low performance."
      ],
      "metadata": {
        "id": "C0I-Wwx0pIcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a few more epochs to train the model better."
      ],
      "metadata": {
        "id": "iMeSpxQihIh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "  training_data_set,\n",
        "  validation_data=validation_data_set,\n",
        "  epochs=5,\n",
        "  verbose=1,\n",
        "  callbacks=[training_logs()]\n",
        ")"
      ],
      "metadata": {
        "id": "Cmtp9mTthP-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if the accuracy improved."
      ],
      "metadata": {
        "id": "eg0qF1ashcd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_predictions()"
      ],
      "metadata": {
        "id": "YcO1jBrkhh6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can repeat the steps above to see if the model improves further.\n",
        "\n",
        "At some point the model may even reach 100% accuracy."
      ],
      "metadata": {
        "id": "um92mnHNkNTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code will display a graph of how accuracy and validation accuacy inceased during training."
      ],
      "metadata": {
        "id": "msDDfBYmyXIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,len(accr)+1),accr),\n",
        "\n",
        "plt.plot(range(20,len(accr)+1,20), val_acc)\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(range(0, len(accr),20), range(1,len(val_acc)+1))\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy at end of batch', 'Validation accuracy at end of epoch'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nE-62XXeYzx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How the trained network 'sees'\n",
        "Below is a visualisation of what one of the model layers learns. This is how filters recognise patterns in the images."
      ],
      "metadata": {
        "id": "iX_JTQgsjwXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_name = 'first'\n",
        "imgh = img_height\n",
        "imgw = img_width\n",
        "layer = model.get_layer(name=layer_name)\n",
        "feature_extractor = tf.keras.Model(inputs=model.inputs, outputs=layer.output)\n",
        "\n",
        "def compute_loss(input_image, filter_index):\n",
        "    activation = feature_extractor(input_image)\n",
        "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
        "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
        "    return tf.reduce_mean(filter_activation)\n",
        "\n",
        "@tf.function\n",
        "def gradient_ascent_step(img, filter_index, learning_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img)\n",
        "        loss = compute_loss(img, filter_index)\n",
        "    # Compute gradients.\n",
        "    grads = tape.gradient(loss, img)\n",
        "    # Normalize gradients.\n",
        "    grads = tf.math.l2_normalize(grads)\n",
        "    img += learning_rate * grads\n",
        "    return loss, img\n",
        "\n",
        "def initialize_image():\n",
        "    for images, labels in training_data_set:\n",
        "        img = images[0] #.numpy().astype(\"float\")\n",
        "        break\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img.shape\n",
        "    # We start from a gray image with some random noise\n",
        "    img = tf.random.uniform((1, imgh, imgw, 1))\n",
        "    return img\n",
        "\n",
        "\n",
        "def visualize_filter(filter_index):\n",
        "    # We run gradient ascent for 20 steps\n",
        "    iterations = 50\n",
        "    learning_rate = 10.0\n",
        "    img = initialize_image()\n",
        "    for iteration in range(iterations):\n",
        "        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n",
        "\n",
        "    # Decode the resulting input image\n",
        "    img = deprocess_image(img[0].numpy())\n",
        "    return loss, img\n",
        "\n",
        "\n",
        "def deprocess_image(img):\n",
        "    # Normalize array: center on 0., ensure variance is 0.15\n",
        "    img -= img.mean()\n",
        "    img /= img.std() + 1e-5\n",
        "    img *= 0.15\n",
        "\n",
        "    # Center crop\n",
        "    img = img[0:25, 0:25, :]\n",
        "\n",
        "    # Clip to [0, 1]\n",
        "    img += 0.5\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    # Convert to RGB array\n",
        "    img *= 255\n",
        "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
        "    return img\n",
        "\n",
        "for i in range(16):\n",
        "  loss, img = visualize_filter(i)\n",
        "  plt.subplot(4,4,i+1)\n",
        "  plt.imshow(img, cmap='gray')\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "w_DK1MOQgByd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full code of artificial neural network\n",
        "\n",
        "Here is the minimal full code for extracting data, creating datasets, defining the artificial neural network model and training the model.\n",
        "\n",
        "As you see, these are onl a few lines of standard code, therefore they would not pass the originality threshold to qualify for copyright protection.\n",
        "\n",
        "Of course, complex models with customised code may become copyright protected, but they should not be just standard call to available functions.\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, utils\n",
        "import pathlib\n",
        "\n",
        "dataset_url = \"https://github.com/stasysdr/dnn-test/raw/main/digits.tgz\"\n",
        "data_dir = utils.get_file('digits.tar', origin=dataset_url, extract=True,cache_dir=\"/content\")\n",
        "data_dir = pathlib.Path(data_dir).with_suffix('')\n",
        "\n",
        "batch_size = 32\n",
        "img_height, img_width = 18, 20\n",
        "\n",
        "training_data_set, validation_data_set = utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"both\",\n",
        "  seed=123,\n",
        "  color_mode='grayscale',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  layers.Rescaling(1./255),\n",
        "  layers.Conv2D(16, 3, activation='relu'),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(64, activation='relu'),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "  training_data_set,\n",
        "  validation_data=validation_data_set,\n",
        "  epochs=5\n",
        ")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PcLlth0N-znZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, utils, Sequential\n",
        "import pathlib\n",
        "\n",
        "dataset_url = \"https://github.com/stasysdr/dnn-test/raw/main/digits.tgz\"\n",
        "data_dir = utils.get_file('digits.tar', origin=dataset_url, extract=True,\n",
        "                          cache_dir=\"/content\")\n",
        "data_dir = pathlib.Path(data_dir).with_suffix('')\n",
        "\n",
        "batch_size = 32\n",
        "img_height, img_width = 18, 20\n",
        "\n",
        "training_data_set, validation_data_set = utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"both\",\n",
        "  seed=123,\n",
        "  color_mode='grayscale',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "model = Sequential([\n",
        "  layers.Rescaling(1./255),\n",
        "  layers.Conv2D(16, 3, activation='relu'),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(64, activation='relu'),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "  training_data_set,\n",
        "  validation_data=validation_data_set,\n",
        "  epochs=15\n",
        ")"
      ],
      "metadata": {
        "id": "jgRdttfTHby9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNm1Ueh0xHEdqf80UyoZOEw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}